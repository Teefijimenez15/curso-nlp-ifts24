Â¡Entendido! Lo harÃ© mÃ¡s sintÃ©tico, conciso y enfocado en los aspectos tÃ©cnicos clave para una entrega de TP de estudiante.



ğŸ“ TP Final Integrador: Sistema RAG y AnÃ¡lisis de ReseÃ±as de Harry Potter

ğŸ¯ Objetivo del Proyecto

Implementar un pipeline integral de PLN que combina tÃ©cnicas tradicionales (clasificaciÃ³n de sentimiento y clustering) con una arquitectura moderna RAG para el anÃ¡lisis y consulta de un corpus de reseÃ±as de la saga Harry Potter.



âš™ï¸ Arquitectura RAG (Retrieval-Augmented Generation)

Este sistema permite hacer consultas en lenguaje natural sobre las reseÃ±as, utilizando la siguiente cadena de herramientas:



OrquestaciÃ³n: LangChain



Splitter: RecursiveCharacterTextSplitter para crear chunks de texto manejables.



Embeddings: Modelo sentence-transformers/all-MiniLM-L6-v2.



Vector Store: ChromaDB (utilizada para persistir y recuperar los embeddings).



LLM (GeneraciÃ³n): google/flan-t5-base (via HuggingFace Pipeline).



ğŸ”¬ AnÃ¡lisis de Machine Learning Tradicional

Se aplicaron tÃ©cnicas supervisadas y no supervisadas para analizar las reseÃ±as.



1\. ClasificaciÃ³n de Sentimiento

Se clasificÃ³ cada reseÃ±a en positivo, negativo o neutro.



VectorizaciÃ³n: TF-IDF.



Modelos Evaluados: Naive Bayes y Random Forest.



Resultado Clave: Ambos modelos alcanzaron una precisiÃ³n de 1.00 en el conjunto de prueba, demostrando una clara separaciÃ³n de las categorÃ­as de sentimiento en el dataset.



2\. Clustering (AgrupaciÃ³n No Supervisada)

TÃ©cnica: K-Means.



Input: Embeddings generados con sentence-transformers.



PropÃ³sito: Identificar grupos semÃ¡nticos naturales dentro del corpus de reseÃ±as.



ğŸ’¾ Dataset y Dependencias

Dataset: ReseÃ±as de Harry Potter (reseÃ±as.csv).



TecnologÃ­as Clave: Python, Pandas, Scikit-learn, LangChain, Transformers, ChromaDB.



EjecuciÃ³n: El proyecto se ejecuta a travÃ©s del notebook Tp\_final\_con\_reseÃ±as\_de\_harry\_potter\_csv.ipynb y puede ser desplegado mediante Gradio/Streamlit.



ğŸ’» Instrucciones RÃ¡pidas

Clonar el repositorio.



Instalar dependencias (pip install -r requirements.txt).



Ejecutar las celdas del notebook en orden para:



Cargar y preprocesar datos.



Entrenar modelos de ML.



Generar y persistir la ChromaDB.



Ejecutar la cadena RAG para consultas.

